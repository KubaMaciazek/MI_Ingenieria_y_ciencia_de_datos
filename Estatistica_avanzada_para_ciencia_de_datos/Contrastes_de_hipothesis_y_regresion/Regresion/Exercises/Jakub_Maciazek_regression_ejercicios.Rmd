---
title: "Estadística avanzada para ciencia de datos"
description: |
  Realization of exercises for the topic of Regression.
author:
  #"Jakub Maciążek"
  name: "Jakub Maciążek </br>"
  affiliation: Faculty of Information and Communication Technology, Wrocław University of Science and Technology
subtitle: "Regresión - Ejercicios"
output:
  rmdformats::downcute:
    use_bookdown: true
    code_folding: show
    self_contained: true
  pdf_document:   
    latex_engine: xelatex   # added to knit to pdf 
  html_document: default    # added to knit to html 
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(readr)

library(knitr)
library(ggplot2)
library(latex2exp)
knitr::opts_chunk$set(fig.align = "center",
                      fig.width = 5,
                      fig.height = 4,
                      collapse = TRUE)
```

# Exercise I
> **ToDo:**
>
 - Import ToyotaCorolla dataset from the course website.
 - Build linear models m1, m2 using Multiple Linear Regression.
 - Analyze the goodness of fit and model quality, explaining the results.
 
 
## Data import and analysys

```{r}
#water_df <- read.csv("S:/0_Universidad_de_Malaga/MI_Ingenieria_y_ciencia_de_datos/Estatistica_avanzada_para_ciencia_de_datos/Contrastes_de_hipothesis_y_regresion/Contraste_de_hypothesis/Ejercicio_2_Tests_estadisticos/water.csv")

#head(water_df)

ToyotaCorolla <- read.csv("S:/0_Universidad_de_Malaga/MI_Ingenieria_y_ciencia_de_datos/Estatistica_avanzada_para_ciencia_de_datos/Contrastes_de_hipothesis_y_regresion/Regresion/Exercises/ToyotaCorolla.csv")
head(ToyotaCorolla)
```

From the analysis of the data set, it can be seen it contains both continuous and categorical variables:


Continuous:

 - price
 - age
 - KM (run kilometers)
 - weight
 
Categorical:

 - FuelType (Diesel, Petrol)
 - MetColor (True, False)
 - Automatic (True, False)
 - Doors (Nr of doors)
 
Variables that theoretically could be continuous, but in practice come from a short range of values, and therefore should be considered as categorical.

 - HP (Horse power)
 - CC (cylinder capacity)


  Regression requires numerical inputs, therefore categorical variables need to be converted into binary or numerical values. In this case all of the variables, except the FuelType, meet this criterion. Therefore FuelType will be converted under the hood by R to dummy binary variables.
  
  
  **Dependent variable data normality** - For given example, it makes sens to build a model predicting car price based on other variables. This makes "price" variable a dependent one, which requires it to be normally distributed and continuous for linear model to work.

```{r}  
shapiro.test(ToyotaCorolla$Price)
hist(ToyotaCorolla$Price)
```

  To test for normality, histogram graph was presented and Shapiro-Wilk test was conducted, which resulted in p-value smaller than 2.2e-16.
  
  From the histogram it can be seen, that data is highly positively skewed. Also, the result of the test is smaller than 0.05, meaning that distribution of variable differs from Normal distribution. Therefore, to receive more accurate prediction it would be useful to either use Generalized Linear Model from for exp. "gamma" family, or to transform the price with logarithmic function, before using standard linear model.
  
  However, knowing the model might be inaccurate due to above reasons, standard linear regression was conducted, as required by the Task.

<!--

Comments for personal knowledge: PYTANIE 1

Więc, chcę budowac model regresji liniowej, ale przewidywana wartość nie jest z rozkładu normalnego:
  - Czy powyższe wnioski poprawne że:
    - lepiej byłoby użyć "Generalized Linear Model from for exp. "gamma" family, or to transform the price with logarithmic function"
    - że to może wpływać na dokładnośc modelu jeśli nic z tym nie zrobię a będę generował regresji liniowej?
  
  - Czy może to żałożeni o rozkłądzie dotyczy independent variables a nie dependent? Powinienem je wszystkie sprawdzić? I co z nimi robić, co wnioskowac jak wyjdą nienormalne?
  - Powinienem przetestować independence of dependent variables?
  - czy potrzebuję zeby dane były ciągle?
  - Powinienem był sprawdzić na wstępnie czy są skorelowane "pearson correlation test'em"?

--> 


## Build of linear model

```{r}
#m1 <- lm(Price ~ Age + KM + FuelType + HP + MetColor + Automatic + Doors + Weight , data = ToyotaCorolla)
#summary(m1)


m1 <- lm(Price ~. , data = ToyotaCorolla)

summary(m1)
```

  Starting analysis with examination of the **F-statistic** and its corresponding **p-value**. It can be observed that its value equals 948 and is far greater than 1, and p-value is essentialy 0, which means that at least one of the predictor variables is significantly related to the outcome variable. To check which predictors are significant, coefficient table was examined.
  
```{r}
summary(m1)$coefficient
```

  Based on the **t-statistic**, it can be seen, that some variables are insignificant, as their Pr(>|t|) is higher than 0.05, meaning there is no significant association between the predictor and the outcome variable.
  
  In order to make a better model, new one was calculated, involving only significant variables (meaning all except MetColor and Doors).
  

```{r}
m2 <- lm(Price ~. -Doors -MetColor  , data = ToyotaCorolla)

summary(m2)
```

  For new model all the predictors are significant, and received model has a following equation:

$Price = \\ -3718 -122.1*Age - 0.01625*KM + 3388*FuelTypeDiesel + 1112*FuelTypePetrol \\  + 60.89*HP + 330.5*Automatic - 4.168*CC + 19.94*Weight$

  Based on DF information, it can be said that it was computed based on 1427 independent pieces of information.


## Model goodness and results analysys

**Example model explanation**

  For the second model m2, following equation was obtained: Price = -3718 -122.1 * Age - 0.01625 * KM + 3388 * FuelTypeDiesel + 1112 * FuelTypePetrol + 60.89 * HP + 330.5 * Automatic - 4.168 * CC + 19.94 * Weight. This means that:
  
 - the value of intercept equals -3718. That is the estimated value of a car that would have value 0 for all of the parameters. In this case it is purely theoretical and has no meaningful interpretation.
 - For each year of car use, its value decreases by 122.1. It also decreases by ~0.016 for every driven kilometer. For new cars, price would be lower by  4.168 for every cubic centimetres capacity of the engine.
 - Value of the car increases by 3388 for Diesels, and by 1112 for Petrol ones, meaning Diesel ones are more expensive by default. Moreover value of the car increases by 60.89 for every HP of engine, and by 19.94 for every kg of weight. Also, car that is Automatic increases value by 330.5.


### Goodness of fit - Adj. R2

  To asses model accuracy, R-squared value can be used. However, it will always increase when more variables are added to the model, even if those variables are only weakly associated with the response.
  
  In this case the models to compare have different number of predictors, therefore another measure, Adjusted R Square, is used to compare their goodness, as it is adjusted to the number of used parameters.
  
  For model m1, value of Adjusted R Squared equals 0.8684, which means that the model explains ~87% of variance when predicting value of the car. Model m2 is slightly better, as for the same parameter has value 0.8685 while using less predictors.
  
   Large proportion of the variability in the response has been explained by the regression.


### Goodness of fit - RSE

  Other measure of the quality of the model is RSE (Residual Standard Error). It is the absolute measure of standard deviation of residual, meaning the average distance between prediction and actual value. Divided by mean value of predicted variable, error rate (relative error) is received.
  
  For m1 and m2 those are respectively 0.1226107 and 0.1225507, which means that both models have ~13% prediction error rate, with m2 being slightly more accurate again.

  
```{r}
rse1 <- sigma(m1)/mean(ToyotaCorolla$Price)
rse1

rse2 <- sigma(m2)/mean(ToyotaCorolla$Price)
rse2
```

<!-- 
Comments for personal knowledge: PYTANIE 2

Źródło: http://www.sthda.com/english/articles/40-regression-analysis/167-simple-linear-regression-in-r/#model-assessment

Dobrze rozumiem ten error rate, że to średni błąd względny predykcji?
--> 


### Model quality - residuals analysis

  In order for the model not to be biased, residual need to have a zero average. In case of model m2, the average is equal to 6.925008e-14, which is essentialy zero.
  
```{r}
mean(residuals(m2))
```  
  
  Errors must also be normally distributed. As can be seen by the histogram and density plot below, they are normally-like distributed. 

<!-- 
Comments for personal knowledge: PYTANIE 3

Residuals should be Normally or Uniformly distributed?
--> 

```{r}
hist(residuals(m2))
plot(density(residuals(m2)))
```

### Model quality - plots analysis: visual verification

**1) Residuals vs fitted values**

  On the plot below, small number of outliers can be seen, however for most of the fitted values, their residuals oscillate symmetrically around the curve close to 0.

```{r}
plot(m2, which = 1)
```

**2) Normal Q-Q**

  This plot illustrates whether errors are normally distributed (in which case they should form a straight line). In this case it is true for most of the values except a couple of outliers.
  Shape of the curve suggests that errors distributions has *fatter tails*, meaning compared to the normal distribution there is more data located at the extremes of the distribution and less data in the center of the distribution. In terms of quantiles this means that the first quantile is much less than the first theoretical quantile and the last quantile is greater than the last theoretical quantile.

```{r}
plot(m2, which = 2)
```

**3) Scale-Location**

  Based on the scale-location plot, we can verify that red line is roughly horizontal, meaning the assumption of homoscedasticity is likely satisfied for a given regression model (spread od residuals is roughly equal for all fitted values).
  
  There is no pattern among residuals, they are randomly scattered around the red line with roughly equal variability at all fitted values. (Although more points can be seen to the left of the plot, it corresponds to the skew in Price data, not residuals themselves.)

```{r}
plot(m2, which = 3)
```

**4) Cook’s distance**

  The last plot shows which points have the greatest influence on the regression (leverage points). In this case those are point 110, 222 and 961.

```{r}
plot(m2, which = 4)
```

  They have great influence on the model, can be detected as outliers, therefore removing these points should increase quality of the model.

```{r}
# https://www.statology.org/how-to-identify-influential-data-points-using-cooks-distance/

#find Cook's distance for each observation in the dataset
cooksD <- cooks.distance(m2)

#identify influential points (traditional threshold 4/n, here value from the graph used -> biggest 3 outliers)
influential_obs <- as.numeric(names(cooksD)[(cooksD >= cooksD[109])])

#define new data frame with influential points removed
outliers_removed <- ToyotaCorolla[-influential_obs, ]

head(outliers_removed)

m2_or <- lm(Price ~. -Doors -MetColor , data = outliers_removed)

summary(m2_or)

plot(m2_or, which = 4)

rse2_or <- sigma(m2_or)/mean(outliers_removed$Price)
rse2_or
```

  As can be see, removing that 3 values had increased Adjusted R Squared from 0.8685 to 0.9366, which means model explains now about 6% more od the variance in predicted values. Also error rate has decreased from ~13% to ~7%.
  
  This is a huge improvement, that as can be seen by new Cook's distance graph might be even increased by removing more outliers. However for now, this one will be used in further analysis replacing m2.
  
```{r}
m2 <- m2_or
```



# Exercise II
> **ToDo:**
>
 - With the ToyotaCorolla dataset, build two polynomial and orthogonal models, and call them m3 and m4.
 - Repeat the analysis made for m1 and m2. Also, analyze the significancy of the regression coefficients.
 
 
## Selection of nonlinear dependent variables
 
 In order to find non-linear dependent variables, multiple scatter plots were generated.

```{r}
plot(ToyotaCorolla$Age, ToyotaCorolla$Price)
plot(ToyotaCorolla$KM, ToyotaCorolla$Price)
plot(ToyotaCorolla$HP, ToyotaCorolla$Price)
plot(ToyotaCorolla$CC, ToyotaCorolla$Price)
plot(ToyotaCorolla$Weight, ToyotaCorolla$Price)
```

Based on this plots following relationships can be observed:
 
 - Age has rather linear influence on the price, slightly exponential
 - KM have exponential effect on Price
 - Both HP and CC are more categorical, and for each category Price comes from specific range. To fit a curve through those lines polynomial could be used. For HP of 3rd degree and for CC of 3rd or 4th degree.
 - For weight, it is hard to observe any pattern, especially due to outliers of very hight weight.
 
  Based on above observations, polynomial aspect will be added to Age, 3rd degree to HP and 3rd to CC.

<!-- 
Comments for personal knowledge: PYTANIE 4

Jaka jest różnica między wielomianowymi a ortogonalnymi?
--> 

## Polynomial model creation and analysis

After examining multiple models, best one was recived for removing of Weigt term and addition of terms: I(CC^2), I(CC^3), I(Age^2), I(Weight^2).

```{r}
m3 <- update(m2,  ~. +I(KM^2) +I(KM^3))

summary(m3)
rse3 <- sigma(m3)/mean(outliers_removed$Price)
rse3
```

```{r}
m3 <- update(m2,  ~. +I(Age^2))

summary(m3)
rse3 <- sigma(m3)/mean(outliers_removed$Price)
rse3
```

```{r}
m3 <- update(m2,  ~. +I(CC^2) +I(CC^3))

summary(m3)
rse3 <- sigma(m3)/mean(outliers_removed$Price)
rse3
```

```{r}
m3 <- update(m2,  ~. +I(CC^2) +I(CC^3) + I(Age^2))

summary(m3)
rse3 <- sigma(m3)/mean(outliers_removed$Price)
rse3
```

```{r}
m3 <- update(m3,  ~. -Weight +I(Weight^2))

summary(m3)
rse3 <- sigma(m3)/mean(outliers_removed$Price)
rse3
```

**Model m3** is based on m2, meaning data with removed outliers, and it's formula is as follows:

$Price = Age + KM + FuelType + HP + Automatic + CC + I(CC^2) + I(CC^3) + I(Age^2) + I(Weight^2)$

Model has **Adj. R^2** value of 0.9389, meaning it predicts **~94%** od variability in outcome data, and has only **~6.5% error rate**.

```{r}
mean(residuals(m3))
hist(residuals(m3))
plot(density(residuals(m3)))
```

There is no bias in the model as errors mean equals 0 and they are normally distributed.


**1) Residuals vs fitted values**

In comparison to m2, number of outliers is smaller and residuals even better oscillate symmetrically around the 0 curve.

```{r}
plot(m3, which = 1)
```

**2) Normal Q-Q**

  Comparing to model m2, errors are better distributed, closer to normal distribution. There was a change in a way how they differ from gaussian distribution. Now distribution has *thin tails*, meaning the first quantiles are occurring at larger than expected values and the last quantiles are occurring at less than expected values.  

```{r}
plot(m3, which = 2)
```

**3) Scale-Location**

  Residuals are more randomly scattered in comparison to model m2, however red line seems to be less horizontal, meaning less satisfied assumption of homoscedasticity. However, this preception might be the result of more zoomed plot than before.

```{r}
plot(m3, which = 3)
```

**4) Cook’s distance**

  New leverage points of great influence were detected: 186, 383, 1048. Their removal may improve the quality of prediction.

```{r}
plot(m3, which = 4)
```

<!-- 

```{r}
#find Cook's distance for each observation in the dataset
cooksD <- cooks.distance(m3)
head(cooksD)
head(names(cooksD))
cooksD["7"]
cooksD[7]

#identify influential points (traditional threshold 4/n, here value from the graph used -> biggest 3 outliers)
influential_obs <- as.numeric(names(cooksD)[(cooksD >= cooksD["186"])])
influential_obs

#define new data frame with influential points removed
outliers_removed <- outliers_removed[-influential_obs, ]

head(outliers_removed)

m3_or <- lm(Price ~. -Doors -MetColor , data = outliers_removed)

summary(m3_or)

plot(m3_or, which = 4)

rse3_or <- sigma(m3_or)/mean(outliers_removed$Price)
rse3_or
```

--> 


## Polynomial orthogonal model

  Model m4 was constructed in a similar way to model m3, but for orthogonal polynomials. The only found meaningful combination was for the modification of m2 model, with following formula:
  
$Price = Age + KM + FuelType + HP + Automatic + CC + poly(Weight, 2)$

```{r}
m4 <- update(m2,  ~. -Weight +poly(Weight, 2))

summary(m4)
rse4 <- sigma(m4)/mean(outliers_removed$Price)
rse4
```

New model m4 has **Adj. R^2** value of 0.9368, meaning it predicts **~94%** od variability in outcome data, and has **~6.6% error rate**, which means that it is less accurate than m3.

```{r}
mean(residuals(m4))
hist(residuals(m4))
plot(density(residuals(m4)))
```

Again, there is no bias in the model as errors mean equals 0 and residuals are normally distributed.


**1) Residuals vs fitted values**

Like in m3, number of outliers is smaller than m2 and residuals better oscillate symmetrically around the 0 curve, although worse than in m3.

```{r}
plot(m4, which = 1)
```

**2) Normal Q-Q**

  Same as in m3, errors are almost normally distributed, with the distribution being more "pointy" than normal one.  

```{r}
plot(m4, which = 2)
```

**3) Scale-Location**

  Residuals are more randomly scattered in comparison to model m2, and red line seems to be more horizontal than m3, meaning assumption of homoscedasticity may be better satisfied.

```{r}
plot(m4, which = 3)
```

**4) Cook’s distance**

  New leverage points of great influence were detected, most importantly 9th one. It is very distinct from other ones in this distribution, and also in comparison to m3 cook's distances. This may mean that it is highly influenced by the weight.

```{r}
plot(m4, which = 4)
```


# Exercise III
> **ToDo:**
>
- Compare all the models built for ToyotaCorolla using ANOVA and explain the results.

anova(m1, m2, m3, m4)

<!--
Comments for personal knowledge: PYTANIE 5

Czy dobrze rozumiem że tu nie możemy tak dokońca użyć ANOV'y, bo:
  - using anova, we can compare situations, where one model extends the other one. In this case can following sets of models can be compared: (m1 with m2 with m3), (m1 with m2 with m4). ... or as m2 has smaller number of arguments than m1... m2 with m1, m2 with m3 and m2 with m4... but can I if i have removed weight argument?

--> 

  Anova method can be used when one model extends another one. In the process of constructing the models, not only new parameters were added, but also data set changed. Therefore, below simplified versions of previous models were retrained for the purpose of comparison.
  
  Of course, this does not reflect comparisons between original models. However, in reality, comparison wouldn't be necessary because in this case model with smaller number of variables is better. Therefore the only importance is to compare m2 with m3 and m4, but this requires adding weight to m3 and m4 or removing it from m2.
  
```{r}
m1t <- lm(Price ~. , data = outliers_removed)

m2t <- lm(Price ~. -Doors -MetColor  , data = outliers_removed)

m3t <- update(m2t,  ~. +I(CC^2) +I(CC^3) + I(Age^2) +I(Weight^2))

m4t <- update(m2t,  ~. +poly(Weight, 2))
```


```{r}
anova(m2t, m1t)

anova(m2t, m3t)

anova(m2t, m4t)
```

  The parameter Pr(>F) is the probability, that rejecting null hypothesis (the most complex model does not fit better than the simplest model) could be an error.          Therefore if the resulting p-value is sufficiently low (usually less than 0.05), we conclude that the more complex model is significantly better than the simpler model, and thus favor the more complex model.
  
  In this case, p-value in all tests was lower than 0.05, meaning m2t model was significantly worse from all other models.
  
  *We cannot compare models m1t, m3t and m4t, because in order to use ANOVA, the second model must be an extended model of the first model, which is not the case here.*

